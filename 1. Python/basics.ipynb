{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c1212b-d07d-42c0-9c62-308771225331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Basic Concepts ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  ss\n",
      "Enter your age:  34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, ss! You are 34 years old.\n",
      "You are an adult.\n",
      "\n",
      "Counting from 1 to 5 using a for loop:\n",
      "1 2 3 4 5 \n",
      "\n",
      "\n",
      "Function Example:\n",
      "Welcome, ss!\n",
      "\n",
      "Available fruits:\n",
      "apple\n",
      "banana\n",
      "cherry\n",
      "\n",
      "Fruit prices:\n",
      "Apple: 100 per kg\n",
      "Banana: 50 per kg\n",
      "Cherry: 150 per kg\n",
      "\n",
      "User data saved to 'user_data.txt'\n",
      "\n",
      "Exception Handling Example:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a number to divide 100:  45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 divided by 45 is 2.2222222222222223\n",
      "\n",
      "=== Object-Oriented Programming ===\n",
      "Hi, I'm ss and I'm 34 years old!\n",
      "\n",
      "=== Using Modules ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a number to calculate its square root:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 3.0 is 1.7320508075688772\n",
      "\n",
      "=== Using Libraries ===\n",
      "Numpy Array: [1 2 3 4 5]\n",
      "Array Squared: [ 1  4  9 16 25]\n",
      "\n",
      "Reading from 'user_data.txt':\n",
      "Name: ss\n",
      "Age: 34\n",
      "\n",
      "\n",
      "=== Lambda Functions ===\n",
      "The square of 5 is 25\n",
      "\n",
      "=== List Comprehensions ===\n",
      "Squares of numbers 1 to 5: [1, 4, 9, 16, 25]\n",
      "\n",
      "=== Generators ===\n",
      "Generated numbers (1 to 5): [1, 2, 3, 4, 5]\n",
      "\n",
      "=== Decorators ===\n",
      "Function is about to be called.\n",
      "This is the decorated function.\n",
      "Function has been called.\n",
      "\n",
      "=== Threading ===\n",
      "Thread: 1\n",
      "Thread: 2\n",
      "Thread: 3\n",
      "Thread: 4\n",
      "Thread: 5\n",
      "\n",
      "=== Program Complete ===\n",
      "Thank you for exploring Python Basics to Advanced!\n"
     ]
    }
   ],
   "source": [
    "# Python Basics to Advanced in One Program\n",
    "\n",
    "# === BASIC CONCEPTS ===\n",
    "\n",
    "# Variables and Data Types\n",
    "print(\"\\n=== Basic Concepts ===\")\n",
    "name = input(\"Enter your name: \")\n",
    "age = int(input(\"Enter your age: \"))\n",
    "print(f\"Hello, {name}! You are {age} years old.\")\n",
    "\n",
    "# Conditional Statements\n",
    "if age < 18:\n",
    "    print(\"You are a minor.\")\n",
    "elif age < 60:\n",
    "    print(\"You are an adult.\")\n",
    "else:\n",
    "    print(\"You are a senior citizen.\")\n",
    "\n",
    "# Loops\n",
    "print(\"\\nCounting from 1 to 5 using a for loop:\")\n",
    "for i in range(1, 6):\n",
    "    print(i, end=\" \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# === FUNCTIONS ===\n",
    "\n",
    "def greet_user(name):\n",
    "    \"\"\"Greets the user with their name.\"\"\"\n",
    "    return f\"Welcome, {name}!\"\n",
    "\n",
    "print(\"\\nFunction Example:\")\n",
    "print(greet_user(name))\n",
    "\n",
    "# === DATA STRUCTURES ===\n",
    "\n",
    "# Lists\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "print(\"\\nAvailable fruits:\")\n",
    "for fruit in fruits:\n",
    "    print(fruit)\n",
    "\n",
    "# Dictionaries\n",
    "fruit_prices = {\"apple\": 100, \"banana\": 50, \"cherry\": 150}\n",
    "print(\"\\nFruit prices:\")\n",
    "for fruit, price in fruit_prices.items():\n",
    "    print(f\"{fruit.capitalize()}: {price} per kg\")\n",
    "\n",
    "# === FILE HANDLING ===\n",
    "\n",
    "try:\n",
    "    with open(\"user_data.txt\", \"w\") as file:\n",
    "        file.write(f\"Name: {name}\\nAge: {age}\\n\")\n",
    "    print(\"\\nUser data saved to 'user_data.txt'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to file: {e}\")\n",
    "\n",
    "# === EXCEPTION HANDLING ===\n",
    "\n",
    "print(\"\\nException Handling Example:\")\n",
    "try:\n",
    "    num = int(input(\"Enter a number to divide 100: \"))\n",
    "    result = 100 / num\n",
    "    print(f\"100 divided by {num} is {result}\")\n",
    "except ZeroDivisionError:\n",
    "    print(\"Error: Division by zero is not allowed.\")\n",
    "except ValueError:\n",
    "    print(\"Error: Please enter a valid number.\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "# === ADVANCED CONCEPTS ===\n",
    "\n",
    "# Classes and Objects\n",
    "class Person:\n",
    "    \"\"\"A simple class to represent a person.\"\"\"\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "    def greet(self):\n",
    "        return f\"Hi, I'm {self.name} and I'm {self.age} years old!\"\n",
    "\n",
    "person = Person(name, age)\n",
    "print(\"\\n=== Object-Oriented Programming ===\")\n",
    "print(person.greet())\n",
    "\n",
    "# Modules and Libraries\n",
    "import math\n",
    "print(\"\\n=== Using Modules ===\")\n",
    "num = float(input(\"Enter a number to calculate its square root: \"))\n",
    "print(f\"The square root of {num} is {math.sqrt(num)}\")\n",
    "\n",
    "# Using Libraries (Numpy Example)\n",
    "print(\"\\n=== Using Libraries ===\")\n",
    "import numpy as np\n",
    "\n",
    "array = np.array([1, 2, 3, 4, 5])\n",
    "print(\"Numpy Array:\", array)\n",
    "print(\"Array Squared:\", array ** 2)\n",
    "\n",
    "# File Handling with Advanced Modes\n",
    "try:\n",
    "    with open(\"user_data.txt\", \"r\") as file:\n",
    "        print(\"\\nReading from 'user_data.txt':\")\n",
    "        print(file.read())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found!\")\n",
    "\n",
    "# === ADVANCED PROGRAMMING ===\n",
    "\n",
    "# Lambda Functions\n",
    "square = lambda x: x ** 2\n",
    "print(\"\\n=== Lambda Functions ===\")\n",
    "print(f\"The square of 5 is {square(5)}\")\n",
    "\n",
    "# List Comprehensions\n",
    "print(\"\\n=== List Comprehensions ===\")\n",
    "squared_numbers = [x ** 2 for x in range(1, 6)]\n",
    "print(f\"Squares of numbers 1 to 5: {squared_numbers}\")\n",
    "\n",
    "# Generators\n",
    "print(\"\\n=== Generators ===\")\n",
    "def generate_numbers(n):\n",
    "    for i in range(1, n + 1):\n",
    "        yield i\n",
    "\n",
    "print(\"Generated numbers (1 to 5):\", list(generate_numbers(5)))\n",
    "\n",
    "# Decorators\n",
    "print(\"\\n=== Decorators ===\")\n",
    "def decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Function is about to be called.\")\n",
    "        func()\n",
    "        print(\"Function has been called.\")\n",
    "    return wrapper\n",
    "\n",
    "@decorator\n",
    "def decorated_function():\n",
    "    print(\"This is the decorated function.\")\n",
    "\n",
    "decorated_function()\n",
    "\n",
    "# Threading\n",
    "print(\"\\n=== Threading ===\")\n",
    "import threading\n",
    "\n",
    "def print_numbers():\n",
    "    for i in range(1, 6):\n",
    "        print(f\"Thread: {i}\")\n",
    "\n",
    "thread = threading.Thread(target=print_numbers)\n",
    "thread.start()\n",
    "thread.join()\n",
    "\n",
    "# === END OF PROGRAM ===\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02dc003-c1be-4b91-b21f-aee157287e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PyTorch Full Program (Basics to Advanced) ===\n",
      "\n",
      "=== Creating Tensors ===\n",
      "Tensor1: tensor([1., 2., 3.])\n",
      "Tensor2 (zeros):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Tensor3 (ones):\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Tensor4 (random):\n",
      "tensor([[0.1332, 0.9346],\n",
      "        [0.5936, 0.8694]])\n",
      "\n",
      "=== Tensor Operations ===\n",
      "Addition: tensor([3., 5., 7.])\n",
      "Multiplication (element-wise): tensor([ 2.,  6., 12.])\n",
      "Dot Product: 20.0\n",
      "Sum of tensor_a: 9.0\n",
      "Mean of tensor_a: 3.0\n",
      "Max of tensor_b: 3.0\n",
      "\n",
      "=== Reshaping Tensors ===\n",
      "Original Tensor (3x3):\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Flattened Tensor: tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "=== Automatic Differentiation (Autograd) ===\n",
      "x: 2.0, Gradient (dy/dx): 7.0\n",
      "\n",
      "=== Dataset and DataLoader ===\n",
      "Batch 1 | Data: tensor([23., 20., 60., 75., 78.,  7., 84., 50., 96., 90.]) | Labels: tensor([ 47.,  41., 121., 151., 157.,  15., 169., 101., 193., 181.])\n",
      "Batch 2 | Data: tensor([88., 29., 36., 70., 14., 46., 82., 63., 79.,  8.]) | Labels: tensor([177.,  59.,  73., 141.,  29.,  93., 165., 127., 159.,  17.])\n",
      "Batch 3 | Data: tensor([85., 27., 37., 48., 71., 91., 21.,  5., 39., 17.]) | Labels: tensor([171.,  55.,  75.,  97., 143., 183.,  43.,  11.,  79.,  35.])\n",
      "\n",
      "=== Defining a Simple Neural Network ===\n",
      "Model Architecture:\n",
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=1, out_features=32, bias=True)\n",
      "  (layer2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "=== Loss Function and Optimizer ===\n",
      "\n",
      "=== Training the Neural Network ===\n",
      "Epoch 1/5 | Loss: 31262900511.7129\n",
      "Epoch 2/5 | Loss: 86545095.5000\n",
      "Epoch 3/5 | Loss: 57785175.0000\n",
      "Epoch 4/5 | Loss: 38585210.5000\n",
      "Epoch 5/5 | Loss: 25769503.2500\n",
      "\n",
      "=== Saving and Loading the Model ===\n",
      "Model saved to 'simple_nn.pth'.\n",
      "Model loaded successfully!\n",
      "\n",
      "=== Inference (Prediction) ===\n",
      "Prediction for input 5.0: -1328.7416\n",
      "\n",
      "=== PyTorch Utilities ===\n",
      "CUDA Available: False\n",
      "Random Tensor with Seed: \n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408]])\n",
      "\n",
      "=== Using Pre-Trained Models ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somu\\AppData\\Local\\Temp\\ipykernel_22280\\2213688764.py:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(\"simple_nn.pth\"))\n",
      "c:\\users\\somu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\users\\somu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\somu/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:03<00:00, 15.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained ResNet18 Model:\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "Modified ResNet18 (Final Layer Changed):\n",
      " ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "=== Transfer Learning ===\n",
      "Transfer Learning Example: Freezing layers except the final layer.\n",
      "\n",
      "=== Using GPU for Training (if available) ===\n",
      "Using device: cpu\n",
      "Prediction on GPU: -1328.7416\n",
      "\n",
      "=== Batch Normalization ===\n",
      "Model with Batch Normalization:\n",
      "SimpleNNWithBN(\n",
      "  (layer1): Linear(in_features=1, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full-Length PyTorch Basics to Advanced Program\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "print(\"\\n=== PyTorch Full Program (Basics to Advanced) ===\")\n",
    "\n",
    "# === Tensors: Basics and Operations ===\n",
    "\n",
    "# Creating Tensors\n",
    "print(\"\\n=== Creating Tensors ===\")\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])  # 1D Tensor\n",
    "tensor2 = torch.zeros((3, 3))            # 3x3 Zero Tensor\n",
    "tensor3 = torch.ones((3, 3))             # 3x3 Ones Tensor\n",
    "tensor4 = torch.rand((2, 2))             # Random 2x2 Tensor\n",
    "\n",
    "print(f\"Tensor1: {tensor1}\")\n",
    "print(f\"Tensor2 (zeros):\\n{tensor2}\")\n",
    "print(f\"Tensor3 (ones):\\n{tensor3}\")\n",
    "print(f\"Tensor4 (random):\\n{tensor4}\")\n",
    "\n",
    "# Tensor Operations: Addition, Multiplication, etc.\n",
    "print(\"\\n=== Tensor Operations ===\")\n",
    "tensor_a = torch.tensor([2.0, 3.0, 4.0])\n",
    "tensor_b = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "print(f\"Addition: {tensor_a + tensor_b}\")\n",
    "print(f\"Multiplication (element-wise): {tensor_a * tensor_b}\")\n",
    "print(f\"Dot Product: {torch.dot(tensor_a, tensor_b)}\")\n",
    "print(f\"Sum of tensor_a: {torch.sum(tensor_a)}\")\n",
    "print(f\"Mean of tensor_a: {torch.mean(tensor_a)}\")\n",
    "print(f\"Max of tensor_b: {torch.max(tensor_b)}\")\n",
    "\n",
    "# Reshaping and Flattening Tensors\n",
    "print(\"\\n=== Reshaping Tensors ===\")\n",
    "tensor5 = torch.arange(1, 10).reshape(3, 3)\n",
    "print(f\"Original Tensor (3x3):\\n{tensor5}\")\n",
    "flattened_tensor = tensor5.flatten()\n",
    "print(f\"Flattened Tensor: {flattened_tensor}\")\n",
    "\n",
    "# Moving Tensors to GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n=== Moving Tensors to GPU ===\")\n",
    "    gpu_tensor = tensor1.to(\"cuda\")\n",
    "    print(f\"Tensor on GPU: {gpu_tensor}\")\n",
    "\n",
    "# === Automatic Differentiation ===\n",
    "\n",
    "print(\"\\n=== Automatic Differentiation (Autograd) ===\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2 + 3 * x + 4\n",
    "y.backward()  # Compute gradients\n",
    "print(f\"x: {x}, Gradient (dy/dx): {x.grad}\")\n",
    "\n",
    "# === Datasets and DataLoader ===\n",
    "\n",
    "print(\"\\n=== Dataset and DataLoader ===\")\n",
    "\n",
    "# Custom Dataset (Simple Linear Dataset)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.arange(1, 101, dtype=torch.float32).reshape(-1, 1)\n",
    "        self.labels = self.data * 2 + 1  # Simple linear relationship\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Viewing data from DataLoader\n",
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1} | Data: {data.squeeze()} | Labels: {labels.squeeze()}\")\n",
    "    if batch_idx == 2:  # Show only 3 batches\n",
    "        break\n",
    "\n",
    "# === Neural Networks (Defining a Model) ===\n",
    "\n",
    "print(\"\\n=== Defining a Simple Neural Network ===\")\n",
    "\n",
    "# Simple Feedforward Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(1, 32)  # Linear layer (input 1 feature -> 32 units)\n",
    "        self.layer2 = nn.Linear(32, 1)  # Linear layer (32 units -> output 1 feature)\n",
    "        self.activation = nn.ReLU()     # ReLU activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "print(f\"Model Architecture:\\n{model}\")\n",
    "\n",
    "# === Loss Function and Optimizer ===\n",
    "\n",
    "print(\"\\n=== Loss Function and Optimizer ===\")\n",
    "# Mean Squared Error Loss (MSE)\n",
    "criterion = nn.MSELoss()\n",
    "# Stochastic Gradient Descent Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# === Training the Neural Network ===\n",
    "\n",
    "print(\"\\n=== Training the Neural Network ===\")\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for data, labels in dataloader:\n",
    "        # Forward pass\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Zero gradients from previous step\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update parameters\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# === Saving and Loading the Model ===\n",
    "\n",
    "print(\"\\n=== Saving and Loading the Model ===\")\n",
    "# Save model state_dict (weights)\n",
    "torch.save(model.state_dict(), \"simple_nn.pth\")\n",
    "print(\"Model saved to 'simple_nn.pth'.\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = SimpleNN()\n",
    "loaded_model.load_state_dict(torch.load(\"simple_nn.pth\"))\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# === Inference (Making Predictions) ===\n",
    "\n",
    "print(\"\\n=== Inference (Prediction) ===\")\n",
    "test_data = torch.tensor([[5.0]])  # Input data for prediction\n",
    "prediction = loaded_model(test_data)\n",
    "print(f\"Prediction for input {test_data.item()}: {prediction.item():.4f}\")\n",
    "\n",
    "# === PyTorch Utilities ===\n",
    "\n",
    "print(\"\\n=== PyTorch Utilities ===\")\n",
    "\n",
    "# Checking for CUDA (GPU availability)\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random_tensor = torch.rand(3, 3)\n",
    "print(f\"Random Tensor with Seed: \\n{random_tensor}\")\n",
    "\n",
    "# === Advanced Topics ===\n",
    "\n",
    "# Using Pre-trained Models (e.g., ResNet18 from torchvision)\n",
    "print(\"\\n=== Using Pre-Trained Models ===\")\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pre-trained ResNet18 model\n",
    "resnet_model = models.resnet18(pretrained=True)\n",
    "print(f\"Pre-trained ResNet18 Model:\\n{resnet_model}\")\n",
    "\n",
    "# Fine-tuning the pre-trained model\n",
    "# For this, you can modify the final layers, but here we'll just demonstrate loading.\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, 2)  # Modify final layer for 2 output classes\n",
    "print(\"Modified ResNet18 (Final Layer Changed):\\n\", resnet_model)\n",
    "\n",
    "# === Transfer Learning ===\n",
    "\n",
    "print(\"\\n=== Transfer Learning ===\")\n",
    "# Example: Freezing all layers except the final one\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "# Unfreeze the final layer\n",
    "for param in resnet_model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Transfer Learning Example: Freezing layers except the final layer.\")\n",
    "\n",
    "# === GPU Acceleration for Model Training ===\n",
    "\n",
    "print(\"\\n=== Using GPU for Training (if available) ===\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "test_data = test_data.to(device)  # Moving test data to the same device\n",
    "prediction_gpu = model(test_data)\n",
    "print(f\"Prediction on GPU: {prediction_gpu.item():.4f}\")\n",
    "\n",
    "# === Batch Normalization (Advanced) ===\n",
    "\n",
    "print(\"\\n=== Batch Normalization ===\")\n",
    "\n",
    "class SimpleNNWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithBN, self).__init__()\n",
    "        self.layer1 = nn.Linear(1, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization\n",
    "        self.layer2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.layer1(x))  # Apply Batch Normalization after first layer\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model_bn = SimpleNNWithBN()\n",
    "print(f\"Model with Batch Normalization:\\n{model_bn}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e41ab-e7c3-4e41-963b-bb718fcc85e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
